{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcr3hLXh9aIOqYjbUy5kwA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menonajayki/LLMs-from-scratch/blob/main/LLM_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYUBQx8ZFqPP",
        "outputId": "c338dda2-f9fa-4e8e-f668-1b5b432cf0b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "c6JV-Z_Q0Muy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UwSiAm-dz3hT"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "XyTaoEenFguD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwK6XNDoFlYP",
        "outputId": "2aa77d36-a1d1-4e68-d0ba-90e193a371d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The model is untrained creating random outs**"
      ],
      "metadata": {
        "id": "ZJ11zJGqHCkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "                       [40,    1107, 588]])   #  \"I really like\"]\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
        "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
        "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)\n",
        "\n",
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(\"Token IDs:\\n\", token_ids)\n",
        "\n",
        "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DoKFqnNGSU5",
        "outputId": "13f66c94-b968-48f1-aad4-584b0db6dec3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n",
            "Token IDs:\n",
            " tensor([[[16657],\n",
            "         [  339],\n",
            "         [42826]],\n",
            "\n",
            "        [[49906],\n",
            "         [29669],\n",
            "         [41751]]])\n",
            "Targets batch 1:  effort moves you\n",
            "Outputs batch 1:  Armed heNetflix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding Loss**"
      ],
      "metadata": {
        "id": "tDf64bGhHSdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 1:\", target_probas_1)\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 2:\", target_probas_2)\n",
        "\n",
        "# Compute logarithm of all token probabilities\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)\n",
        "\n",
        "# Calculate the average probability for each token\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)\n",
        "\n",
        "# CEL: torch already have a function for this\n",
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08wJ6vzDHQqX",
        "outputId": "4196ec82-1209-48f6-867a-8a9ac5a005ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
            "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n",
            "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
            "tensor(-10.7940)\n",
            "tensor(10.7940)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "\n",
        "# Targets have shape (batch_size, num_tokens)\n",
        "print(\"Targets shape:\", targets.shape)\n",
        "\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "\n",
        "print(\"Flattened logits:\", logits_flat.shape)\n",
        "print(\"Flattened targets:\", targets_flat.shape)\n",
        "\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)\n",
        "\n",
        "# Perplexity\n",
        "perplexity = torch.exp(loss)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au2XR9yRHtvI",
        "outputId": "ce9c1169-3531-4196-ec23-7213499900ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 3, 50257])\n",
            "Targets shape: torch.Size([2, 3])\n",
            "Flattened logits: torch.Size([6, 50257])\n",
            "Flattened targets: torch.Size([6])\n",
            "tensor(10.7940)\n",
            "tensor(48725.8203)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the LLM**"
      ],
      "metadata": {
        "id": "r7N18LZ8IQCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "\n",
        "# First 100 characters\n",
        "print(text_data[:99])\n",
        "\n",
        "# Last 100 characters\n",
        "print(text_data[-99:])\n",
        "\n",
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBuXWyyVIOsw",
        "outputId": "ce64690b-83ec-4f5f-8065-6cdca585ef6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
            "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n",
            "Characters: 20479\n",
            "Tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # reproducibility seed\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "7jOdN5jFQ7Xm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ],
      "metadata": {
        "id": "gpyvCMEbRjgG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "print(\"Training tokens:\", train_tokens)\n",
        "print(\"Validation tokens:\", val_tokens)\n",
        "print(\"All tokens:\", train_tokens + val_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHVCGK6uRnzN",
        "outputId": "c4b6dd2e-371b-4e91-9f97-dba905e8c724"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "Training tokens: 4608\n",
            "Validation tokens: 512\n",
            "All tokens: 5120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "tHYe9HpVRvaf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LEsoRUYR0B-",
        "outputId": "3589a081-f42d-4b73-dbea-3b9653bdf11f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.987583266364204\n",
            "Validation loss: 10.981104850769043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A simple training function and training**"
      ],
      "metadata": {
        "id": "-UZVHx6dSsw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "wYif_1OrSXAO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-jFf5IZSeUY",
        "outputId": "eb301f9a-f58e-4327-af76-db083fbc4021"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.818, Val loss 9.930\n",
            "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.336\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.623, Val loss 7.053\n",
            "Ep 2 (Step 000015): Train loss 6.047, Val loss 6.605\n",
            "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
            "Ep 3 (Step 000020): Train loss 5.532, Val loss 6.507\n",
            "Ep 3 (Step 000025): Train loss 5.399, Val loss 6.389\n",
            "Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had the, and, and, and, and, and, and, and, and, and\n",
            "Ep 4 (Step 000030): Train loss 4.895, Val loss 6.280\n",
            "Ep 4 (Step 000035): Train loss 4.648, Val loss 6.304\n",
            "Every effort moves you.  \"I the picture.                    \"I\"I the picture\"I had the the honour of the picture and I had been the picture of\n",
            "Ep 5 (Step 000040): Train loss 4.023, Val loss 6.165\n",
            "Every effort moves you know                                                 \n",
            "Ep 6 (Step 000045): Train loss 3.625, Val loss 6.172\n",
            "Ep 6 (Step 000050): Train loss 3.045, Val loss 6.144\n",
            "Every effort moves you know the was his a little the.  \"I had the last word.           \"Oh, and I had a little.   \"I looked, and I had a little of\n",
            "Ep 7 (Step 000055): Train loss 2.948, Val loss 6.183\n",
            "Ep 7 (Step 000060): Train loss 2.230, Val loss 6.128\n",
            "Every effort moves you know the picture to have been too--I felt, and Mrs.  \"I was no--and the fact, and that, and I was his pictures.  \"I looked up his pictures--and--because he was a little\n",
            "Ep 8 (Step 000065): Train loss 1.774, Val loss 6.162\n",
            "Ep 8 (Step 000070): Train loss 1.475, Val loss 6.229\n",
            "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"I looked up, and the fact, and to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
            "Ep 9 (Step 000075): Train loss 1.135, Val loss 6.268\n",
            "Ep 9 (Step 000080): Train loss 0.858, Val loss 6.298\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.    \"I looked, and that, and I remember getting off a prodigious phrase about the honour being _mine_--because he's the first\n",
            "Ep 10 (Step 000085): Train loss 0.627, Val loss 6.382\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Training completed in 0.58 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loss**"
      ],
      "metadata": {
        "id": "pGTCY845S5En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "4pnz43NXSrwz",
        "outputId": "48d421ca-32d8-488d-8324-0d3e924ea5e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Function with Temperature Scaling and Top K Sampling**"
      ],
      "metadata": {
        "id": "W5LN8Ojozvkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "rkYUIOt2zhQP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "U-42y3Kzz-iN",
        "outputId": "c4a7669f-b995-46b8-8105-22cb280b49cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b033f13285eb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m token_ids = generate(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_to_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Every effort moves you\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-2df101796b2e>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, idx, max_new_tokens, context_size, temperature, top_k, eos_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0midx_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2a1a4dd23e54>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtok_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mpos_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_embeds\u001b[0m  \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the Model**"
      ],
      "metadata": {
        "id": "w8qRmp400QPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "W8_ZZEanz_PQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Model**"
      ],
      "metadata": {
        "id": "ZFU9qC8Z0UCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ],
      "metadata": {
        "id": "xta4FAb-0Mwb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Pretrained Weights**"
      ],
      "metadata": {
        "id": "yhT_OZ1q0nBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow tqdm --quiet"
      ],
      "metadata": {
        "id": "ykReq-D70sqo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT download functions**"
      ],
      "metadata": {
        "id": "-WJQzZxNcJd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path)\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "\n",
        "def download_file(url, destination):\n",
        "    # Send a GET request to download the file\n",
        "\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            # Get the total file size from headers, defaulting to 0 if not present\n",
        "            file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "            # Check if file exists and has the same size\n",
        "            if os.path.exists(destination):\n",
        "                file_size_local = os.path.getsize(destination)\n",
        "                if file_size == file_size_local:\n",
        "                    print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                    return\n",
        "\n",
        "            # Define the block size for reading the file\n",
        "            block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "            # Initialize the progress bar with total file size\n",
        "            progress_bar_description = os.path.basename(url)  # Extract filename from URL\n",
        "            with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "                # Open the destination file in binary write mode\n",
        "                with open(destination, \"wb\") as file:\n",
        "                    # Read the file in chunks and write to destination\n",
        "                    while True:\n",
        "                        chunk = response.read(block_size)\n",
        "                        if not chunk:\n",
        "                            break\n",
        "                        file.write(chunk)\n",
        "                        progress_bar.update(len(chunk))  # Update progress bar\n",
        "    except urllib.error.HTTPError:\n",
        "        s = (\n",
        "            f\"The specified URL ({url}) is incorrect, the internet connection cannot be established,\"\n",
        "            \"\\nor the requested file is temporarily unavailable.\\nPlease visit the following website\"\n",
        "            \" for help: https://github.com/rasbt/LLMs-from-scratch/discussions/273\")\n",
        "        print(s)\n",
        "\n",
        "\n",
        "# Alternative way using `requests`\n",
        "\"\"\"\n",
        "def download_file(url, destination):\n",
        "    # Send a GET request to download the file in streaming mode\n",
        "    response = requests.get(url, stream=True)\n",
        "\n",
        "    # Get the total file size from headers, defaulting to 0 if not present\n",
        "    file_size = int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "    # Check if file exists and has the same size\n",
        "    if os.path.exists(destination):\n",
        "        file_size_local = os.path.getsize(destination)\n",
        "        if file_size == file_size_local:\n",
        "            print(f\"File already exists and is up-to-date: {destination}\")\n",
        "            return\n",
        "\n",
        "    # Define the block size for reading the file\n",
        "    block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "    # Initialize the progress bar with total file size\n",
        "    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
        "    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "        # Open the destination file in binary write mode\n",
        "        with open(destination, \"wb\") as file:\n",
        "            # Iterate over the file data in chunks\n",
        "            for chunk in response.iter_content(block_size):\n",
        "                progress_bar.update(len(chunk))  # Update progress bar\n",
        "                file.write(chunk)  # Write the chunk to the file\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "0KEufCJtbce_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download using the functions    -->    gpt2/124M**"
      ],
      "metadata": {
        "id": "Uka-1Ms7cUMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09mD5OE4cOz4",
        "outputId": "484ffdce-affb-4870-c0b0-401bed151402"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 180kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.36MiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 63.0kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:21<00:00, 22.8MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 5.12MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.24MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.25MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data and test\n",
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())\n",
        "print(params[\"wte\"])\n",
        "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_lmMshqcmpJ",
        "outputId": "4090ffb2-8cc1-472d-f43d-21948f7f0b58"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
            "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
            "   0.04531523]\n",
            " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
            "   0.04318958]\n",
            " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
            "  -0.08785918]\n",
            " ...\n",
            " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
            "  -0.06952604]\n",
            " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
            "  -0.02245961]\n",
            " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
            "   0.12067825]]\n",
            "Token embedding weight tensor dimensions: (50257, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model configurations in a dictionary for compactness\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt2-small (124M)\"  # Example model name\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ],
      "metadata": {
        "id": "_1g5RHGYc4cm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# OpenAI weight to GPT Model Assignment\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "# Loading the weights\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "# Call functions and load the weights\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ],
      "metadata": {
        "id": "UksXmOKgc9RX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the model is loaded correctly**"
      ],
      "metadata": {
        "id": "V7rVDa7ydU6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oir6USR7dSPg",
        "outputId": "2af2d02a-b4a2-4e1b-bd65-98521744c82c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
            "\n",
            "This would remove you from a battle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Visualization**"
      ],
      "metadata": {
        "id": "hSOKjGNjCTGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz --quiet"
      ],
      "metadata": {
        "id": "4FlddZ97_HDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "# Configuration dictionary\n",
        "cfg = {\n",
        "    \"vocab_size\": 30522,\n",
        "    \"emb_dim\": 768,\n",
        "    \"context_length\": 128,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"n_layers\": 12,\n",
        "    \"n_heads\": 12,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "model = GPTModel(cfg)\n",
        "\n",
        "# Generate a random input tensor\n",
        "input_tensor = torch.randint(0, cfg[\"vocab_size\"], (1, cfg[\"context_length\"]))\n",
        "\n",
        "# Get the model output\n",
        "output = model(input_tensor)\n",
        "\n",
        "# Visualize the model architecture\n",
        "dot = make_dot(output, params=dict(model.named_parameters()))\n",
        "dot.format = 'png'  # You can set this to 'svg' or 'pdf' as well\n",
        "dot.render('gpt_model_architecture')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "39fufHNQ-3Qg",
        "outputId": "6e39fdd6-f266-40ef-8a86-a51493d965cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.85191 to fit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gpt_model_architecture.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}